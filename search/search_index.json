{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CORAL implementation for ordinal regression with deep neural networks. About CORAL, short for COnsistent RAnk Logits, is a method for ordinal regression with deep neural networks, which addresses the rank inconsistency issue of other ordinal regression frameworks. Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks. This repository implements the CORAL functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" in the upper left menu bar. If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: https://github.com/Raschka-research-group/coral-cnn . Cite as If you use BioPandas as part of your workflow in a scientific publication, please consider citing the BioPandas repository with the following DOI: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . @article{coral2020, title = \"Rank consistent ordinal regression for neural networks with application to age estimation\", journal = \"Pattern Recognition Letters\", volume = \"140\", pages = \"325 - 331\", year = \"2020\", issn = \"0167-8655\", doi = \"https://doi.org/10.1016/j.patrec.2020.11.008\", url = \"http://www.sciencedirect.com/science/article/pii/S016786552030413X\", author = \"Wenzhi Cao and Vahid Mirjalili and Sebastian Raschka\", keywords = \"Deep learning, Ordinal regression, Convolutional neural networks, Age prediction, Machine learning, Biometrics\" }","title":"Home"},{"location":"#about","text":"CORAL, short for COnsistent RAnk Logits, is a method for ordinal regression with deep neural networks, which addresses the rank inconsistency issue of other ordinal regression frameworks. Originally, developed this method in the context of age prediction from face images. Our approach was evaluated on several face image datasets for age prediction using ResNet-34, but it is compatible with other state-of-the-art deep neural networks. This repository implements the CORAL functionality (neural network layer, loss function, and dataset utilities) for convenient use. Examples are provided via the \"Tutorials\" in the upper left menu bar. If you are looking for the orginal implementation, training datasets, and training log files corresponding to the paper, you can find these here: https://github.com/Raschka-research-group/coral-cnn .","title":"About"},{"location":"#cite-as","text":"If you use BioPandas as part of your workflow in a scientific publication, please consider citing the BioPandas repository with the following DOI: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . @article{coral2020, title = \"Rank consistent ordinal regression for neural networks with application to age estimation\", journal = \"Pattern Recognition Letters\", volume = \"140\", pages = \"325 - 331\", year = \"2020\", issn = \"0167-8655\", doi = \"https://doi.org/10.1016/j.patrec.2020.11.008\", url = \"http://www.sciencedirect.com/science/article/pii/S016786552030413X\", author = \"Wenzhi Cao and Vahid Mirjalili and Sebastian Raschka\", keywords = \"Deep learning, Ordinal regression, Convolutional neural networks, Age prediction, Machine learning, Biometrics\" }","title":"Cite as"},{"location":"CHANGELOG/","text":"Release Notes The changelog for the current development version is available at [https://github.com/rasbt/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/rasbt/coral_pytorch/blob/main/docs/CHANGELOG.md. 1.0.0 (11/15/2020) Downloads Source code (zip) Source code (tar.gz) New Features First release. Changes First release. Bug Fixes First release.","title":"Release Notes"},{"location":"CHANGELOG/#release-notes","text":"The changelog for the current development version is available at [https://github.com/rasbt/coral_pytorch/blob/main/docs/CHANGELOG.md](https://github.com/rasbt/coral_pytorch/blob/main/docs/CHANGELOG.md.","title":"Release Notes"},{"location":"CHANGELOG/#100-11152020","text":"","title":"1.0.0 (11/15/2020)"},{"location":"CHANGELOG/#downloads","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features","text":"First release.","title":"New Features"},{"location":"CHANGELOG/#changes","text":"First release.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes","text":"First release.","title":"Bug Fixes"},{"location":"citing/","text":"If you use BioPandas as part of your workflow in a scientific publication, please consider citing the BioPandas repository with the following DOI: Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka (2020). Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation . Pattern Recognition Letters 140, pp. 325-331; https://doi.org/10.1016/j.patrec.2020.11.008 . @article{coral2020, title = \"Rank consistent ordinal regression for neural networks with application to age estimation\", journal = \"Pattern Recognition Letters\", volume = \"140\", pages = \"325 - 331\", year = \"2020\", issn = \"0167-8655\", doi = \"https://doi.org/10.1016/j.patrec.2020.11.008\", url = \"http://www.sciencedirect.com/science/article/pii/S016786552030413X\", author = \"Wenzhi Cao and Vahid Mirjalili and Sebastian Raschka\", keywords = \"Deep learning, Ordinal regression, Convolutional neural networks, Age prediction, Machine learning, Biometrics\" }","title":"Citing"},{"location":"installation/","text":"Installing coral_pytorch Requirements BioPandas requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0 PyPI You can install the latest stable release of coral_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install coral_pytorch Latest GitHub Source Code You want to try out the latest features before they go live on PyPI? Install the coral_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/coral_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Installation"},{"location":"installation/#installing-coral_pytorch","text":"","title":"Installing coral_pytorch"},{"location":"installation/#requirements","text":"BioPandas requires the following software and packages: Python >= 3.6 PyTorch >= 1.5.0","title":"Requirements"},{"location":"installation/#pypi","text":"You can install the latest stable release of coral_pytorch directly from Python's package index via pip by executing the following code from your command line: pip install coral_pytorch","title":"PyPI"},{"location":"installation/#latest-github-source-code","text":"You want to try out the latest features before they go live on PyPI? Install the coral_pytorch dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/coral_pytorch.git Alternatively, you download the package manually from GitHub via the Dowload ZIP button, unzip it, navigate into the package directory, and execute the following command: python setup.py install","title":"Latest GitHub Source Code"},{"location":"license/","text":"MIT License Copyright (c) 2020 Sebastian Raschka Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright (c) 2020 Sebastian Raschka Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"api_modules/coral_pytorch.dataset/label_to_levels/","text":"label_to_levels label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"Label to levels"},{"location":"api_modules/coral_pytorch.dataset/label_to_levels/#label_to_levels","text":"label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"label_to_levels"},{"location":"api_modules/coral_pytorch.dataset/levels_from_labelbatch/","text":"levels_from_labelbatch levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"Levels from labelbatch"},{"location":"api_modules/coral_pytorch.dataset/levels_from_labelbatch/#levels_from_labelbatch","text":"levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"levels_from_labelbatch"},{"location":"api_modules/coral_pytorch.layers/CoralLayer/","text":"CoralLayer CoralLayer(size_in, num_classes) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset.","title":"CoralLayer"},{"location":"api_modules/coral_pytorch.layers/CoralLayer/#corallayer","text":"CoralLayer(size_in, num_classes) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset.","title":"CoralLayer"},{"location":"api_modules/coral_pytorch.losses/coral_loss/","text":"coral_loss coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"Coral loss"},{"location":"api_modules/coral_pytorch.losses/coral_loss/#coral_loss","text":"coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_loss"},{"location":"api_subpackages/coral_pytorch.dataset/","text":"coral_pytorch version: 0.1.0dev label_to_levels label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.]) levels_from_labelbatch levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"coral_pytorch.dataset"},{"location":"api_subpackages/coral_pytorch.dataset/#label_to_levels","text":"label_to_levels(label, num_classes, dtype=torch.float32) Converts integer class label to extended binary label vector Parameters label : int Class label to be converted into a extended binary vector. Should be smaller than num_classes-1. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_classes-1,) Extended binary label vector. Type is determined by the dtype parameter. Examples >>> label_to_levels(0, num_classes=5) tensor([0., 0., 0., 0.]) >>> label_to_levels(1, num_classes=5) tensor([1., 0., 0., 0.]) >>> label_to_levels(3, num_classes=5) tensor([1., 1., 1., 0.]) >>> label_to_levels(4, num_classes=5) tensor([1., 1., 1., 1.])","title":"label_to_levels"},{"location":"api_subpackages/coral_pytorch.dataset/#levels_from_labelbatch","text":"levels_from_labelbatch(labels, num_classes, dtype=torch.float32) Converts a list of integer class label to extended binary label vectors Parameters labels : list or 1D orch.tensor, shape=(num_labels,) A list or 1D torch.tensor with integer class labels to be converted into extended binary label vectors. num_classes : int The number of class clabels in the dataset. Assumes class labels start at 0. Determines the size of the output vector. dtype : torch data type (default=torch.float32) Data type of the torch output vector for the extended binary labels. Returns levels : torch.tensor, shape=(num_labels, num_classes-1) Examples >>> levels_from_labelbatch(labels=[2, 1, 4], num_classes=5) tensor([[1., 1., 0., 0.], [1., 0., 0., 0.], [1., 1., 1., 1.]])","title":"levels_from_labelbatch"},{"location":"api_subpackages/coral_pytorch.layers/","text":"coral_pytorch version: 0.1.0dev CoralLayer CoralLayer(size_in, num_classes) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset.","title":"coral_pytorch.layers"},{"location":"api_subpackages/coral_pytorch.layers/#corallayer","text":"CoralLayer(size_in, num_classes) Implements CORAL layer described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters size_in : int Number of input features for the inputs to the forward method, which are expected to have shape=(num_examples, num_features). num_classes : int Number of classes in the dataset.","title":"CoralLayer"},{"location":"api_subpackages/coral_pytorch.losses/","text":"coral_pytorch version: 0.1.0dev coral_loss coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_pytorch.losses"},{"location":"api_subpackages/coral_pytorch.losses/#coral_loss","text":"coral_loss(logits, levels, importance_weights=None, reduction='mean') Computes the CORAL loss described in Cao, Mirjalili, and Raschka (2020) *Rank Consistent Ordinal Regression for Neural Networks with Application to Age Estimation* Pattern Recognition Letters, https://doi.org/10.1016/j.patrec.2020.11.008 Parameters logits : torch.tensor, shape(num_examples, num_classes-1) Outputs of the CORAL layer. levels : torch.tensor, shape(num_examples, num_classes-1) True labels represented as extended binary vectors (via coral_pytorch.dataset.levels_from_labelbatch ). importance_weights : torch.tensor, shape=(num_classes-1,) (default=None) Optional weights for the different labels in levels. A tensor of ones, i.e., torch.ones(num_classes-1, dtype=torch.float32) will result in uniform weights that have the same effect as None. reduction : str or None (default='mean') If 'mean' or 'sum', returns the averaged or summed loss value across all data points (rows) in logits. If None, returns a vector of shape (num_examples,) Returns loss : torch.tensor A torch.tensor containing a single loss value (if reduction='mean' or ' sum' ) or a loss value for each data record (if reduction=None ). Examples >>> import torch >>> levels = torch.tensor( ... [[1., 1., 0., 0.], ... [1., 0., 0., 0.], ... [1., 1., 1., 1.]]) >>> logits = torch.tensor( ... [[2.1, 1.8, -2.1, -1.8], ... [1.9, -1., -1.5, -1.3], ... [1.9, 1.8, 1.7, 1.6]]) >>> coral_loss(logits, levels) tensor(0.6920)","title":"coral_loss"},{"location":"tutorials/mnist/","text":"CORAL CNN for predicting handwritten digits (MNIST) This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps. 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print('Training on', DEVICE) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True) test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor()) train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=True, shuffle=True) test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, drop_last=True, shuffle=False) # Checking the dataset for images, labels in train_loader: print('Image batch dimensions:', images.shape) print('Image label dimensions:', labels.shape) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128]) 2 - Equipping CNN with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Using the Sequential API, we specify the CORAl layer as self.fc = CoralLayer(size_in=294, num_classes=num_classes) This is because the convolutional and pooling layers torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2)), torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2))) produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function logits = self.fc(x) probas = torch.sigmoid(logits) please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class ConvNet(torch.nn.Module): def __init__(self, num_classes): super(ConvNet, self).__init__() self.features = torch.nn.Sequential( torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2)), torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2))) ### Specify CORAL layer self.fc = CoralLayer(size_in=294, num_classes=num_classes) ###--------------------------------------------------------------------### def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) # flatten ##### Use CORAL layer ##### logits = self.fc(x) probas = torch.sigmoid(logits) ###--------------------------------------------------------------------### return logits, probas torch.manual_seed(random_seed) model = ConvNet(num_classes=NUM_CLASSES) model.to(DEVICE) optimizer = torch.optim.Adam(model.parameters()) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) 2) Apply the CORAL loss (also provided via coral_pytorch ): cost = coral_loss(logits, levels) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range(num_epochs): model = model.train() for batch_idx, (features, class_labels) in enumerate(train_loader): ##### Convert class labels for CORAL levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) ###--------------------------------------------------------------------### features = features.to(DEVICE) levels = levels.to(DEVICE) logits, probas = model(features) #### CORAL loss cost = coral_loss(logits, levels) ###--------------------------------------------------------------------### optimizer.zero_grad() cost.backward() optimizer.step() ### LOGGING if not batch_idx % 200: print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost)) Epoch: 001/010 | Batch 000/468 | Cost: 6.2250 Epoch: 001/010 | Batch 200/468 | Cost: 4.5538 Epoch: 001/010 | Batch 400/468 | Cost: 4.1572 Epoch: 002/010 | Batch 000/468 | Cost: 4.2916 Epoch: 002/010 | Batch 200/468 | Cost: 3.7469 Epoch: 002/010 | Batch 400/468 | Cost: 3.7111 Epoch: 003/010 | Batch 000/468 | Cost: 3.5796 Epoch: 003/010 | Batch 200/468 | Cost: 3.2361 Epoch: 003/010 | Batch 400/468 | Cost: 3.1930 Epoch: 004/010 | Batch 000/468 | Cost: 3.2449 Epoch: 004/010 | Batch 200/468 | Cost: 2.9884 Epoch: 004/010 | Batch 400/468 | Cost: 2.7252 Epoch: 005/010 | Batch 000/468 | Cost: 2.9845 Epoch: 005/010 | Batch 200/468 | Cost: 2.7993 Epoch: 005/010 | Batch 400/468 | Cost: 2.6468 Epoch: 006/010 | Batch 000/468 | Cost: 2.7458 Epoch: 006/010 | Batch 200/468 | Cost: 2.4976 Epoch: 006/010 | Batch 400/468 | Cost: 2.5533 Epoch: 007/010 | Batch 000/468 | Cost: 2.6634 Epoch: 007/010 | Batch 200/468 | Cost: 2.5637 Epoch: 007/010 | Batch 400/468 | Cost: 2.3448 Epoch: 008/010 | Batch 000/468 | Cost: 2.3006 Epoch: 008/010 | Batch 200/468 | Cost: 2.7393 Epoch: 008/010 | Batch 400/468 | Cost: 2.1759 Epoch: 009/010 | Batch 000/468 | Cost: 2.3998 Epoch: 009/010 | Batch 200/468 | Cost: 2.2425 Epoch: 009/010 | Batch 400/468 | Cost: 2.1656 Epoch: 010/010 | Batch 000/468 | Cost: 2.3247 Epoch: 010/010 | Batch 200/468 | Cost: 2.3120 Epoch: 010/010 | Batch 400/468 | Cost: 2.4770 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse(model, data_loader, device): with torch.no_grad(): mae, mse, acc, num_examples = 0., 0., 0., 0 for i, (features, targets) in enumerate(data_loader): features = features.to(device) targets = targets.float().to(device) logits, probas = model(features) predicted_labels = proba_to_label(probas).float() num_examples += targets.size(0) mae += torch.sum(torch.abs(predicted_labels - targets)) mse += torch.sum((predicted_labels - targets)**2) mae = mae / num_examples mse = mse / num_examples return mae, mse train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE) test_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE) print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}') print(f'Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}') Mean absolute error (train/test): 0.90 | 0.91 Mean squared error (train/test): 1.84 | 1.87 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"MNIST"},{"location":"tutorials/mnist/#coral-cnn-for-predicting-handwritten-digits-mnist","text":"This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression. Please note that MNIST is not an ordinal dataset . The reason why we use MNIST in this tutorial is that it is included in the PyTorch's torchvision library and is thus easy to work with, since it doesn't require extra data downloading and preprocessing steps.","title":"CORAL CNN for predicting handwritten digits (MNIST)"},{"location":"tutorials/mnist/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders. This is a general procedure that is not specific to CORAL. import torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.05 num_epochs = 10 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print('Training on', DEVICE) ########################## ### MNIST DATASET ########################## # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True) test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor()) train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=True, shuffle=True) test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, drop_last=True, shuffle=False) # Checking the dataset for images, labels in train_loader: print('Image batch dimensions:', images.shape) print('Image label dimensions:', labels.shape) break Training on cpu Image batch dimensions: torch.Size([128, 1, 28, 28]) Image label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/mnist/#2-equipping-cnn-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a convolutional neural network for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Using the Sequential API, we specify the CORAl layer as self.fc = CoralLayer(size_in=294, num_classes=num_classes) This is because the convolutional and pooling layers torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2)), torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2))) produce a flattened feature vector of 294 units. Then, when using the CORAL layer in the forward function logits = self.fc(x) probas = torch.sigmoid(logits) please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class ConvNet(torch.nn.Module): def __init__(self, num_classes): super(ConvNet, self).__init__() self.features = torch.nn.Sequential( torch.nn.Conv2d(1, 3, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2)), torch.nn.Conv2d(3, 6, (3, 3), (1, 1), 1), torch.nn.MaxPool2d((2, 2), (2, 2))) ### Specify CORAL layer self.fc = CoralLayer(size_in=294, num_classes=num_classes) ###--------------------------------------------------------------------### def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) # flatten ##### Use CORAL layer ##### logits = self.fc(x) probas = torch.sigmoid(logits) ###--------------------------------------------------------------------### return logits, probas torch.manual_seed(random_seed) model = ConvNet(num_classes=NUM_CLASSES) model.to(DEVICE) optimizer = torch.optim.Adam(model.parameters())","title":"2 - Equipping CNN with CORAL layer"},{"location":"tutorials/mnist/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) 2) Apply the CORAL loss (also provided via coral_pytorch ): cost = coral_loss(logits, levels) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range(num_epochs): model = model.train() for batch_idx, (features, class_labels) in enumerate(train_loader): ##### Convert class labels for CORAL levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) ###--------------------------------------------------------------------### features = features.to(DEVICE) levels = levels.to(DEVICE) logits, probas = model(features) #### CORAL loss cost = coral_loss(logits, levels) ###--------------------------------------------------------------------### optimizer.zero_grad() cost.backward() optimizer.step() ### LOGGING if not batch_idx % 200: print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost)) Epoch: 001/010 | Batch 000/468 | Cost: 6.2250 Epoch: 001/010 | Batch 200/468 | Cost: 4.5538 Epoch: 001/010 | Batch 400/468 | Cost: 4.1572 Epoch: 002/010 | Batch 000/468 | Cost: 4.2916 Epoch: 002/010 | Batch 200/468 | Cost: 3.7469 Epoch: 002/010 | Batch 400/468 | Cost: 3.7111 Epoch: 003/010 | Batch 000/468 | Cost: 3.5796 Epoch: 003/010 | Batch 200/468 | Cost: 3.2361 Epoch: 003/010 | Batch 400/468 | Cost: 3.1930 Epoch: 004/010 | Batch 000/468 | Cost: 3.2449 Epoch: 004/010 | Batch 200/468 | Cost: 2.9884 Epoch: 004/010 | Batch 400/468 | Cost: 2.7252 Epoch: 005/010 | Batch 000/468 | Cost: 2.9845 Epoch: 005/010 | Batch 200/468 | Cost: 2.7993 Epoch: 005/010 | Batch 400/468 | Cost: 2.6468 Epoch: 006/010 | Batch 000/468 | Cost: 2.7458 Epoch: 006/010 | Batch 200/468 | Cost: 2.4976 Epoch: 006/010 | Batch 400/468 | Cost: 2.5533 Epoch: 007/010 | Batch 000/468 | Cost: 2.6634 Epoch: 007/010 | Batch 200/468 | Cost: 2.5637 Epoch: 007/010 | Batch 400/468 | Cost: 2.3448 Epoch: 008/010 | Batch 000/468 | Cost: 2.3006 Epoch: 008/010 | Batch 200/468 | Cost: 2.7393 Epoch: 008/010 | Batch 400/468 | Cost: 2.1759 Epoch: 009/010 | Batch 000/468 | Cost: 2.3998 Epoch: 009/010 | Batch 200/468 | Cost: 2.2425 Epoch: 009/010 | Batch 400/468 | Cost: 2.1656 Epoch: 010/010 | Batch 000/468 | Cost: 2.3247 Epoch: 010/010 | Batch 200/468 | Cost: 2.3120 Epoch: 010/010 | Batch 400/468 | Cost: 2.4770","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/mnist/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse(model, data_loader, device): with torch.no_grad(): mae, mse, acc, num_examples = 0., 0., 0., 0 for i, (features, targets) in enumerate(data_loader): features = features.to(device) targets = targets.float().to(device) logits, probas = model(features) predicted_labels = proba_to_label(probas).float() num_examples += targets.size(0) mae += torch.sum(torch.abs(predicted_labels - targets)) mse += torch.sum((predicted_labels - targets)**2) mae = mae / num_examples mse = mse / num_examples return mae, mse train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE) test_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE) print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}') print(f'Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}') Mean absolute error (train/test): 0.90 | 0.91 Mean squared error (train/test): 1.84 | 1.87 Note that MNIST is not an ordinal dataset (there is no order between the image categories), so computing the MAE or MSE doesn't really make sense but we use it anyways for demonstration purposes.","title":"4 -- Evaluate model"},{"location":"tutorials/poker/","text":"CORAL MLP for predicting poker hands This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression in the context of predicting poker hands. 0 -- Obtaining and preparing the Poker Hand dataset from the UCI ML repository First, we are going to download and prepare the UCI Poker Hand dataset from https://archive.ics.uci.edu/ml/datasets/Poker+Hand and save it as CSV files locally. This is a general procedure that is not specific to CORAL. This dataset has 10 ordinal labels, 0: Nothing in hand; not a recognized poker hand 1: One pair; one pair of equal ranks within five cards 2: Two pairs; two pairs of equal ranks within five cards 3: Three of a kind; three equal ranks within five cards 4: Straight; five cards, sequentially ranked with no gaps 5: Flush; five cards with the same suit 6: Full house; pair + different rank three of a kind 7: Four of a kind; four equal ranks within five cards 8: Straight flush; straight + flush 9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush where 0 < 1 < 2 ... < 9. Download training examples and test dataset: import pandas as pd train_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\", header=None) train_features = train_df.loc[:, 0:10] train_labels = train_df.loc[:, 10] print('Number of features:', train_features.shape[1]) print('Number of training examples:', train_features.shape[0]) Number of features: 11 Number of training examples: 25010 test_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\", header=None) test_df.head() test_features = test_df.loc[:, 0:10] test_labels = test_df.loc[:, 10] print('Number of test examples:', test_features.shape[0]) Number of test examples: 1000000 Standardize features: from sklearn.preprocessing import StandardScaler sc = StandardScaler() train_features_sc = sc.fit_transform(train_features) test_features_sc = sc.transform(test_features) Save training and test set as CSV files locally pd.DataFrame(train_features_sc).to_csv('train_features.csv', index=False) train_labels.to_csv('train_labels.csv', index=False) pd.DataFrame(test_features_sc).to_csv('test_features.csv', index=False) test_labels.to_csv('test_labels.csv', index=False) # don't need those anymore del test_features del train_features del train_labels del test_labels 1 -- Setting up the dataset and dataloader In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print('Training on', DEVICE) Training on cpu from torch.utils.data import Dataset import numpy as np class MyDataset(Dataset): def __init__(self, csv_path_features, csv_path_labels, dtype=np.float32): self.features = pd.read_csv(csv_path_features).values.astype(np.float32) self.labels = pd.read_csv(csv_path_labels).values.flatten() def __getitem__(self, index): inputs = self.features[index] label = self.labels[index] return inputs, label def __len__(self): return self.labels.shape[0] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset('train_features.csv', 'train_labels.csv') test_dataset = MyDataset('test_features.csv', 'test_labels.csv') train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, # want to shuffle the dataset num_workers=0) # number processes/CPUs to use test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, # want to shuffle the dataset num_workers=0) # number processes/CPUs to use # Checking the dataset for inputs, labels in train_loader: print('Input batch dimensions:', inputs.shape) print('Input label dimensions:', labels.shape) break Input batch dimensions: torch.Size([128, 11]) Input label dimensions: torch.Size([128]) 2 - Equipping MLP with CORAL layer In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class CoralMLP(torch.nn.Module): def __init__(self, num_classes): super(CoralMLP, self).__init__() self.features = torch.nn.Sequential( torch.nn.Linear(11, 5), torch.nn.Linear(5, 5)) ### Specify CORAL layer self.fc = CoralLayer(size_in=5, num_classes=num_classes) ###--------------------------------------------------------------------### def forward(self, x): x = self.features(x) ##### Use CORAL layer ##### logits = self.fc(x) probas = torch.sigmoid(logits) ###--------------------------------------------------------------------### return logits, probas torch.manual_seed(random_seed) model = CoralMLP(num_classes=NUM_CLASSES) model.to(DEVICE) optimizer = torch.optim.Adam(model.parameters()) 3 - Using the CORAL loss for model training During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) 2) Apply the CORAL loss (also provided via coral_pytorch ): cost = coral_loss(logits, levels) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range(num_epochs): model = model.train() for batch_idx, (features, class_labels) in enumerate(train_loader): ##### Convert class labels for CORAL levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) ###--------------------------------------------------------------------### features = features.to(DEVICE) levels = levels.to(DEVICE) logits, probas = model(features) #### CORAL loss cost = coral_loss(logits, levels) ###--------------------------------------------------------------------### optimizer.zero_grad() cost.backward() optimizer.step() ### LOGGING if not batch_idx % 200: print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost)) Epoch: 001/020 | Batch 000/196 | Cost: 6.5905 Epoch: 002/020 | Batch 000/196 | Cost: 3.0309 Epoch: 003/020 | Batch 000/196 | Cost: 1.7885 Epoch: 004/020 | Batch 000/196 | Cost: 1.2904 Epoch: 005/020 | Batch 000/196 | Cost: 1.2604 Epoch: 006/020 | Batch 000/196 | Cost: 1.3774 Epoch: 007/020 | Batch 000/196 | Cost: 1.0882 Epoch: 008/020 | Batch 000/196 | Cost: 1.1178 Epoch: 009/020 | Batch 000/196 | Cost: 1.0749 Epoch: 010/020 | Batch 000/196 | Cost: 1.0276 Epoch: 011/020 | Batch 000/196 | Cost: 0.9430 Epoch: 012/020 | Batch 000/196 | Cost: 0.9547 Epoch: 013/020 | Batch 000/196 | Cost: 0.7979 Epoch: 014/020 | Batch 000/196 | Cost: 0.9496 Epoch: 015/020 | Batch 000/196 | Cost: 0.6845 Epoch: 016/020 | Batch 000/196 | Cost: 0.9132 Epoch: 017/020 | Batch 000/196 | Cost: 0.8270 Epoch: 018/020 | Batch 000/196 | Cost: 0.6653 Epoch: 019/020 | Batch 000/196 | Cost: 0.6094 Epoch: 020/020 | Batch 000/196 | Cost: 0.7930 4 -- Evaluate model Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse(model, data_loader, device): with torch.no_grad(): mae, mse, acc, num_examples = 0., 0., 0., 0 for i, (features, targets) in enumerate(data_loader): features = features.to(device) targets = targets.float().to(device) logits, probas = model(features) predicted_labels = proba_to_label(probas).float() num_examples += targets.size(0) mae += torch.sum(torch.abs(predicted_labels - targets)) mse += torch.sum((predicted_labels - targets)**2) mae = mae / num_examples mse = mse / num_examples return mae, mse train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE) test_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE) print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}') print(f'Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}') Mean absolute error (train/test): 0.10 | 0.10 Mean squared error (train/test): 0.21 | 0.21","title":"Poker Hands"},{"location":"tutorials/poker/#coral-mlp-for-predicting-poker-hands","text":"This tutorial explains how to equip a deep neural network with the CORAL layer and loss function for ordinal regression in the context of predicting poker hands.","title":"CORAL MLP for predicting poker hands"},{"location":"tutorials/poker/#0-obtaining-and-preparing-the-poker-hand-dataset-from-the-uci-ml-repository","text":"First, we are going to download and prepare the UCI Poker Hand dataset from https://archive.ics.uci.edu/ml/datasets/Poker+Hand and save it as CSV files locally. This is a general procedure that is not specific to CORAL. This dataset has 10 ordinal labels, 0: Nothing in hand; not a recognized poker hand 1: One pair; one pair of equal ranks within five cards 2: Two pairs; two pairs of equal ranks within five cards 3: Three of a kind; three equal ranks within five cards 4: Straight; five cards, sequentially ranked with no gaps 5: Flush; five cards with the same suit 6: Full house; pair + different rank three of a kind 7: Four of a kind; four equal ranks within five cards 8: Straight flush; straight + flush 9: Royal flush; {Ace, King, Queen, Jack, Ten} + flush where 0 < 1 < 2 ... < 9. Download training examples and test dataset: import pandas as pd train_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\", header=None) train_features = train_df.loc[:, 0:10] train_labels = train_df.loc[:, 10] print('Number of features:', train_features.shape[1]) print('Number of training examples:', train_features.shape[0]) Number of features: 11 Number of training examples: 25010 test_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\", header=None) test_df.head() test_features = test_df.loc[:, 0:10] test_labels = test_df.loc[:, 10] print('Number of test examples:', test_features.shape[0]) Number of test examples: 1000000 Standardize features: from sklearn.preprocessing import StandardScaler sc = StandardScaler() train_features_sc = sc.fit_transform(train_features) test_features_sc = sc.transform(test_features) Save training and test set as CSV files locally pd.DataFrame(train_features_sc).to_csv('train_features.csv', index=False) train_labels.to_csv('train_labels.csv', index=False) pd.DataFrame(test_features_sc).to_csv('test_features.csv', index=False) test_labels.to_csv('test_labels.csv', index=False) # don't need those anymore del test_features del train_features del train_labels del test_labels","title":"0 -- Obtaining and preparing the Poker Hand dataset from the UCI ML repository"},{"location":"tutorials/poker/#1-setting-up-the-dataset-and-dataloader","text":"In this section, we set up the data set and data loaders using PyTorch utilities. This is a general procedure that is not specific to CORAL. import torch ########################## ### SETTINGS ########################## # Hyperparameters random_seed = 1 learning_rate = 0.001 num_epochs = 20 batch_size = 128 # Architecture NUM_CLASSES = 10 # Other DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") print('Training on', DEVICE) Training on cpu from torch.utils.data import Dataset import numpy as np class MyDataset(Dataset): def __init__(self, csv_path_features, csv_path_labels, dtype=np.float32): self.features = pd.read_csv(csv_path_features).values.astype(np.float32) self.labels = pd.read_csv(csv_path_labels).values.flatten() def __getitem__(self, index): inputs = self.features[index] label = self.labels[index] return inputs, label def __len__(self): return self.labels.shape[0] import torch from torch.utils.data import DataLoader # Note transforms.ToTensor() scales input images # to 0-1 range train_dataset = MyDataset('train_features.csv', 'train_labels.csv') test_dataset = MyDataset('test_features.csv', 'test_labels.csv') train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, # want to shuffle the dataset num_workers=0) # number processes/CPUs to use test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, # want to shuffle the dataset num_workers=0) # number processes/CPUs to use # Checking the dataset for inputs, labels in train_loader: print('Input batch dimensions:', inputs.shape) print('Input label dimensions:', labels.shape) break Input batch dimensions: torch.Size([128, 11]) Input label dimensions: torch.Size([128])","title":"1 -- Setting up the dataset and dataloader"},{"location":"tutorials/poker/#2-equipping-mlp-with-coral-layer","text":"In this section, we are using the CoralLayer implemented in coral_pytorch to outfit a multilayer perceptron for ordinal regression. Note that the CORAL method only requires replacing the last (output) layer, which is typically a fully-connected layer, by the CORAL layer. Also, please use the sigmoid not softmax function (since the CORAL method uses a concept known as extended binary classification as described in the paper). from coral_pytorch.layers import CoralLayer class CoralMLP(torch.nn.Module): def __init__(self, num_classes): super(CoralMLP, self).__init__() self.features = torch.nn.Sequential( torch.nn.Linear(11, 5), torch.nn.Linear(5, 5)) ### Specify CORAL layer self.fc = CoralLayer(size_in=5, num_classes=num_classes) ###--------------------------------------------------------------------### def forward(self, x): x = self.features(x) ##### Use CORAL layer ##### logits = self.fc(x) probas = torch.sigmoid(logits) ###--------------------------------------------------------------------### return logits, probas torch.manual_seed(random_seed) model = CoralMLP(num_classes=NUM_CLASSES) model.to(DEVICE) optimizer = torch.optim.Adam(model.parameters())","title":"2 - Equipping MLP with CORAL layer"},{"location":"tutorials/poker/#3-using-the-coral-loss-for-model-training","text":"During training, all you need to do is to 1) convert the integer class labels into the extended binary label format using the levels_from_labelbatch provided via coral_pytorch : levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) 2) Apply the CORAL loss (also provided via coral_pytorch ): cost = coral_loss(logits, levels) from coral_pytorch.dataset import levels_from_labelbatch from coral_pytorch.losses import coral_loss for epoch in range(num_epochs): model = model.train() for batch_idx, (features, class_labels) in enumerate(train_loader): ##### Convert class labels for CORAL levels = levels_from_labelbatch(class_labels, num_classes=NUM_CLASSES) ###--------------------------------------------------------------------### features = features.to(DEVICE) levels = levels.to(DEVICE) logits, probas = model(features) #### CORAL loss cost = coral_loss(logits, levels) ###--------------------------------------------------------------------### optimizer.zero_grad() cost.backward() optimizer.step() ### LOGGING if not batch_idx % 200: print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost)) Epoch: 001/020 | Batch 000/196 | Cost: 6.5905 Epoch: 002/020 | Batch 000/196 | Cost: 3.0309 Epoch: 003/020 | Batch 000/196 | Cost: 1.7885 Epoch: 004/020 | Batch 000/196 | Cost: 1.2904 Epoch: 005/020 | Batch 000/196 | Cost: 1.2604 Epoch: 006/020 | Batch 000/196 | Cost: 1.3774 Epoch: 007/020 | Batch 000/196 | Cost: 1.0882 Epoch: 008/020 | Batch 000/196 | Cost: 1.1178 Epoch: 009/020 | Batch 000/196 | Cost: 1.0749 Epoch: 010/020 | Batch 000/196 | Cost: 1.0276 Epoch: 011/020 | Batch 000/196 | Cost: 0.9430 Epoch: 012/020 | Batch 000/196 | Cost: 0.9547 Epoch: 013/020 | Batch 000/196 | Cost: 0.7979 Epoch: 014/020 | Batch 000/196 | Cost: 0.9496 Epoch: 015/020 | Batch 000/196 | Cost: 0.6845 Epoch: 016/020 | Batch 000/196 | Cost: 0.9132 Epoch: 017/020 | Batch 000/196 | Cost: 0.8270 Epoch: 018/020 | Batch 000/196 | Cost: 0.6653 Epoch: 019/020 | Batch 000/196 | Cost: 0.6094 Epoch: 020/020 | Batch 000/196 | Cost: 0.7930","title":"3 - Using the CORAL loss for model training"},{"location":"tutorials/poker/#4-evaluate-model","text":"Finally, after model training, we can evaluate the performance of the model. For example, via the mean absolute error and mean squared error measures. For this, we are going to use the proba_to_label utility function from coral_pytorch to convert the probabilities back to the orginal label. from coral_pytorch.dataset import proba_to_label def compute_mae_and_mse(model, data_loader, device): with torch.no_grad(): mae, mse, acc, num_examples = 0., 0., 0., 0 for i, (features, targets) in enumerate(data_loader): features = features.to(device) targets = targets.float().to(device) logits, probas = model(features) predicted_labels = proba_to_label(probas).float() num_examples += targets.size(0) mae += torch.sum(torch.abs(predicted_labels - targets)) mse += torch.sum((predicted_labels - targets)**2) mae = mae / num_examples mse = mse / num_examples return mae, mse train_mae, train_mse = compute_mae_and_mse(model, train_loader, DEVICE) test_mae, test_mse = compute_mae_and_mse(model, test_loader, DEVICE) print(f'Mean absolute error (train/test): {train_mae:.2f} | {test_mae:.2f}') print(f'Mean squared error (train/test): {train_mse:.2f} | {test_mse:.2f}') Mean absolute error (train/test): 0.10 | 0.10 Mean squared error (train/test): 0.21 | 0.21","title":"4 -- Evaluate model"}]}